# Episode 03 - References

This folder contains academic papers, research articles, and additional reading materials referenced in Episode 03. These resources provide comprehensive coverage of learning dynamics in multi-agent systems, from self-play mechanisms to policy gradient methods and mean field approximations for large-scale coordination.

## ðŸ“– **Key Papers**

### **Self-Play & Evolutionary Training**

**Vinyals, O., et al. (2019).** "Grandmaster level in StarCraft II using multi-agent reinforcement learning." *Nature*, 575(7782), 350-354. DOI: 10.1038/s41586-019-1724-z

**Filippos, C., et al. (2020).** "Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning." *NeurIPS 2020*. [arxiv.org/abs/1909.07613](https://arxiv.org/abs/2006.07169)

**McMahan, H. B., et al. (2003).** "Planning in the presence of cost functions controlled by an adversary." *ICML 2003*. [https://dl.acm.org/doi/10.5555/3041838.3041895](https://dl.acm.org/doi/10.5555/3041838.3041906)

**Lanctot, M., et al. (2017).** "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning." *NeurIPS 2017*. [https://arxiv.org/abs/1711.00832](https://arxiv.org/abs/1711.00832)

### **Policy Gradients & Learning Dynamics**

**Lowe, R., et al. (2017).** "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments." *NeurIPS 2017*. [https://arxiv.org/abs/1706.02275](https://arxiv.org/abs/1706.02275)

**Foerster, J., et al. (2018).** "Counterfactual Multi-Agent Policy Gradients." *AAAI 2018*. [https://dl.acm.org/doi/10.5555/3504035.3504398](https://dl.acm.org/doi/10.5555/3504035.3504398)

**Yu, C., et al. (2022).** "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games." *NeurIPS 2022*. [https://arxiv.org/abs/2103.01955](https://dl.acm.org/doi/abs/10.5555/3600270.3602057)

**Zhang, K., et al. (2018).** "Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms." *Handbook of Reinforcement Learning and Control*. [https://arxiv.org/abs/1911.10635](https://link.springer.com/chapter/10.1007/978-3-030-60990-0_12)

### **Mean Field Theory & Large-Scale Systems**

**Yang, Y., et al. (2018).** "Mean Field Multi-Agent Reinforcement Learning." *ICML 2018*. [https://arxiv.org/abs/1802.05438](https://arxiv.org/abs/1802.05438)

**Guo, X., et al. (2019).** "Learning Mean-Field Games." *NeurIPS 2019*. [https://arxiv.org/abs/1901.09585](https://arxiv.org/abs/1901.09585)

**Anahtarci, B., et al. (2019).** "Q-Learning in Regularized Mean-field Games." *Dynamic Games and Applications*, 11(1), 132-148. DOI: 10.1007/s13235-019-00327-4

## ðŸ“š **Essential Reading**

### **Foundational Theory**

**Brown, G. W. (1951).** "Iterative solution of games by fictitious play." *Activity analysis of production and allocation*, 13(1), 374-376.

**Tan, M. (1993).** "Multi-agent reinforcement learning: Independent vs. cooperative agents." *ICML 1993*. [https://dl.acm.org/doi/10.5555/3091529.3091654](https://dl.acm.org/doi/10.5555/3091529.3091572)

**Littman, M. L. (1994).** "Markov games as a framework for multi-agent reinforcement learning." *ICML 1994*. [https://dl.acm.org/doi/10.5555/3091574.3091594](https://dl.acm.org/doi/10.5555/3091574.3091594)

### **Advanced Methods**

**Eccles, T., et al. (2019).** "Biases for Emergent Communication in Multi-agent Reinforcement Learning." *NeurIPS 2019*. [https://arxiv.org/abs/1912.05676](https://dl.acm.org/doi/10.5555/3454287.3455463)

**Papoudakis, G., et al. (2021).** "Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks." *NeurIPS 2021*. [https://arxiv.org/abs/2006.07869](https://arxiv.org/pdf/2006.07869)

**Wen, Y., et al. (2022).** "Multi-Agent Reinforcement Learning is a Sequence Modeling Problem." *NeurIPS 2022*. [https://arxiv.org/abs/2205.14953](https://arxiv.org/abs/2205.14953)

## ðŸ’» **Implementation Resources**

**PyMARL Framework**: Multi-agent reinforcement learning framework with policy gradient implementations. [https://github.com/oxwhirl/pymarl](https://github.com/oxwhirl/pymarl)

**EPyMARL**: Extended PyMARL with MAPPO and advanced algorithms. [https://github.com/uoe-agents/epymarl](https://github.com/uoe-agents/epymarl)

**MADDPG Official**: OpenAI's original MADDPG implementation. [https://github.com/openai/maddpg](https://github.com/openai/maddpg)

**PettingZoo**: Multi-agent environment suite for testing learning dynamics. [https://github.com/Farama-Foundation/PettingZoo](https://github.com/Farama-Foundation/PettingZoo)

## ðŸ”¬ **Advanced Research**

**Muller, J., et al. (2021).** "Policy Gradient Methods for Multi-Agent Reinforcement Learning with Imperfect Information." *ICLR 2021*. https://openreview.net/forum?id=WGqk0X4qHv

**Liu, I., et al. (2020).** "Emergent Tool Use From Multi-Agent Autocurricula." *ICLR 2020*. https://arxiv.org/abs/1909.07528

**Eccles, T., et al. (2022).** "Universal Successor Features Approximators." *ICML 2022*. https://arxiv.org/abs/2110.14645

**Mguni, D., et al. (2018).** "Coordinating the Crowd: Inducing Desirable Equilibria in Non-Cooperative Systems." *AAMAS 2018*. DOI: 10.5555/3237383.3237916

---

*These references provide theoretical foundations and practical implementations for understanding how multi-agent systems learn, adapt, and converge in dynamic environments.*
