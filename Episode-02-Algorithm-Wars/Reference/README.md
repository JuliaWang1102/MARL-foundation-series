# Episode 02 - References
This folder contains academic papers, research articles, and additional reading materials referenced in Episode 02. These resources provide comprehensive coverage of MARL paradigm design patterns, from centralized to decentralized approaches, along with their representative algorithms and implementations.

ðŸ“– Key Papers

Boutilier, C. (1996). "Planning and acting in partially observable stochastic domains." TARK '96.
DOI: 10.1016/0004-3702(95)00082-3

Sunehag, P., et al. (2017). "Value-Decomposition Networks For Cooperative Multi-Agent Learning." ArXiv preprint.
https://arxiv.org/abs/1706.05296

Rashid, T., et al. (2018). "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning." ICML 2018.
https://arxiv.org/abs/1803.11485

Lowe, R., et al. (2017). "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments." NeurIPS 2017.
https://arxiv.org/abs/1706.02275

Foerster, J., et al. (2018). "Counterfactual Multi-Agent Policy Gradients." AAAI 2018.
https://arxiv.org/abs/1705.08926

Das, A., et al. (2019). "TarMAC: Targeted Multi-Agent Communication." ICML 2019.
https://arxiv.org/abs/1810.11187

Iqbal, S., & Sha, F. (2019). "Multi-Actor-Attention-Critic for Mixed Cooperative-Competitive Multi-Agent Reinforcement Learning." ICML 2019.
https://arxiv.org/abs/1810.02912

Wang, T., et al. (2020). "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles." ICML 2020.
https://arxiv.org/abs/2003.08039

He, H., et al. (2016). "Opponent Modeling in Deep Reinforcement Learning." ICML 2016.
https://arxiv.org/abs/1609.05559

Hu, J., & Wellman, M. P. (2003). "Nash Q-Learning for Multi-Agent Stochastic Games." Journal of Machine Learning Research, 4, 1039-1069.
https://www.jmlr.org/papers/v4/hu03a.html

Mao, W., et al. (2023). "Multi-Agent Meta-Reinforcement Learning: Sharper Convergence Rates with Task Similarity." NeurIPS 2023.
https://proceedings.neurips.cc/paper_files/paper/2023/file/d1b1a091088904cbc7f7faa2b45c8f36-Paper-Conference.pdf

ðŸ“š Essential Reading
Yu, C., et al. (2022). "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games." NeurIPS 2022.
https://arxiv.org/abs/2103.01955

Son, K., et al. (2019). "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning." ICML 2019.
https://arxiv.org/abs/1905.05408

Singh, A., et al. (2019). "Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks." ICLR 2019.
https://arxiv.org/abs/1812.09755

ðŸ’» Implementation Resources
PyMARL Framework: The primary multi-agent reinforcement learning framework containing implementations of most algorithms discussed.
https://github.com/oxwhirl/pymarl

PyMARL2: Optimized version with improved performance and additional algorithms.
https://github.com/hijkzzz/pymarl2

EPyMARL: Extended PyMARL with additional environments and algorithms.
https://github.com/uoe-agents/epymarl

Minimal-MARL: Educational implementations for learning MARL concepts.
https://github.com/koulanurag/minimal-marl

MADDPG Official: OpenAI's official implementation of MADDPG.
https://github.com/openai/maddpg

MAAC Official: Multi-Actor-Attention-Critic implementation.
https://github.com/shariqiqbal2810/MAAC

ROMA Official: Role-Oriented Multi-Agent implementation.
https://github.com/TonghanWang/ROMA

ðŸ”¬ Additional Research
Tampuu, A., et al. (2017). "Multiagent cooperation and competition with deep reinforcement learning." PLoS ONE.
DOI: 10.1371/journal.pone.0172395

Wang, T., et al. (2021). "Rode: Learning roles to decompose multi-agent tasks." ICLR 2021.
https://arxiv.org/abs/2010.01523

Malucelli, N., et al. (2024). "Neighbor-Based Decentralized Training Strategies for Multi-Agent Reinforcement Learning." ACM TAAS.
DOI: 10.1145/3672608.3707923

Wang, J., et al. (2021). "QPLEX: Duplex Dueling Multi-Agent Q-Learning." ICLR 2021.
https://openreview.net/forum?id=RcmkOxxIQV
Wang, J., et al. (2021). "QPLEX: Duplex Dueling Multi-Agent Q-Learning." ICLR 2021.
OpenReview
