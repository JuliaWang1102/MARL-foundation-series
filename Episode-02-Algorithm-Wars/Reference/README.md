# Episode 02 - References
This folder contains academic papers, research articles, and additional reading materials referenced in Episode 02. These resources provide comprehensive coverage of MARL paradigm design patterns, from centralized to decentralized approaches, along with their representative algorithms and implementations.

ðŸ“– Key Papers
CTCE (Centralized Training, Centralized Execution)
Boutilier, C. (1996). "Planning and acting in partially observable stochastic domains." TARK '96.
Paper Link

CTDE (Centralized Training, Decentralized Execution)
Sunehag, P., et al. (2017). "Value-Decomposition Networks For Cooperative Multi-Agent Learning." ArXiv preprint.
arXiv Paper

Rashid, T., et al. (2018). "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning." ICML 2018.
arXiv Paper

Lowe, R., et al. (2017). "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments." NeurIPS 2017.
arXiv Paper

Foerster, J., et al. (2018). "Counterfactual Multi-Agent Policy Gradients." AAAI 2018.
arXiv Paper

Das, A., et al. (2019). "TarMAC: Targeted Multi-Agent Communication." ICML 2019.
arXiv Paper

Iqbal, S., & Sha, F. (2019). "Multi-Actor-Attention-Critic for Mixed Cooperative-Competitive Multi-Agent Reinforcement Learning." ICML 2019.
arXiv Paper

Wang, T., et al. (2020). "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles." ICML 2020.
arXiv Paper

DTDE (Decentralized Training, Decentralized Execution)
He, H., et al. (2016). "Opponent Modeling in Deep Reinforcement Learning." ICML 2016.
arXiv Paper

Hu, J., & Wellman, M. P. (2003). "Nash Q-Learning for Multi-Agent Stochastic Games." Journal of Machine Learning Research, 4, 1039-1069.
JMLR Paper

Mao, W., et al. (2023). "Multi-Agent Meta-Reinforcement Learning: Sharper Convergence Rates with Task Similarity." NeurIPS 2023.
NeurIPS Paper

ðŸ“š Essential Reading
Yu, C., et al. (2022). "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games." NeurIPS 2022.
arXiv Paper

Son, K., et al. (2019). "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning." ICML 2019.
arXiv Paper

Singh, A., et al. (2019). "Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks." ICLR 2019.
arXiv Paper

ðŸ’» Implementation Resources
PyMARL Framework: The primary multi-agent reinforcement learning framework containing implementations of most algorithms discussed.
GitHub Repository

PyMARL2: Optimized version with improved performance and additional algorithms.
GitHub Repository

EPyMARL: Extended PyMARL with additional environments and algorithms.
GitHub Repository

Minimal-MARL: Educational implementations for learning MARL concepts.
GitHub Repository

MADDPG Official: OpenAI's official implementation of MADDPG.
GitHub Repository

MAAC Official: Multi-Actor-Attention-Critic implementation.
GitHub Repository

ROMA Official: Role-Oriented Multi-Agent implementation.
GitHub Repository

ðŸ”¬ Additional Research
Tampuu, A., et al. (2017). "Multiagent cooperation and competition with deep reinforcement learning." PLoS ONE.
Paper Link

Wang, T., et al. (2021). "Rode: Learning roles to decompose multi-agent tasks." ICLR 2021.
arXiv Paper

Malucelli, N., et al. (2024). "Neighbor-Based Decentralized Training Strategies for Multi-Agent Reinforcement Learning." ACM TAAS.
Paper Link

Wang, J., et al. (2021). "QPLEX: Duplex Dueling Multi-Agent Q-Learning." ICLR 2021.
OpenReview
